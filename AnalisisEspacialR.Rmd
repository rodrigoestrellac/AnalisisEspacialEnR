---
title: "Herramientas de análisis espacial en R"
author: "Martin Montane"
description: "Usando R para analizar datos espaciales"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook
documentclass: book
---
# Herramientas de análisis espacial en R {-}

Los datos espaciales requieren un tratamiento particular tanto en su representación, su almacenamiento, sus transformaciones, sus visualizaciones y su análisis. En este compendio de notas de clase se introducen las herramientas necesarias para comenzar a aprovechar todas las oportunidades que los datos espaciales nos brindan.

## ¿Qué necesitamos para arrancar? {-}

En estas notas de clase se usa el lenguaje de programación R. Recomiendo utilizar RStudio para que nos ayude con los proyectos y la edición de los códigos en R Descargar instalar estos dos softwares es muy simple ya que son gratuitos. R de hecho es un lenguaje de programación *open source*, o de código abierto, lo que significa que cualquiera puede colaborar. [Haciendo click aquí](https://cran.r-project.org/) van a poder descargar la última versión de R para Windows, Mac o Linux. Una vez que lo hayan descargado solo tienen que instalarlo.

Ahora [descarguen RStudio](https://www.rstudio.com/products/rstudio/download/), también van a poder elegir la versión que corresponde según su sistema operativo. RStudio va a identificar automáticamente la versión de R que ya tienen instalada, **por lo que es importante que instalen RStudio luego de haber instalado R**. Una vez que tienen todo esto instalado pueden pasar al primer capítulo de este libro

## Qué deberían saber para aprovechar al máximo este libro {-}

En este libro hacemos 

<!--chapter:end:index.Rmd-->

# Datos espaciales en R

```
Al terminar este capítulo ustedes van a poder:
- Comprender por qué los datos espaciales son distintos al resto de los datos
- Las dificultades de la representación de esos datos y los estándares utilizados
- Trabajar con datos espaciales en R: su importación, manipulación e introducción a los gráficos
- Identificar los principales tipos de archivos donde suelen compartirse estos datos
```
## ¿Qué es un dato espacial?

Un dato **espacial** o **georreferenciado** tiene una característica que lo hace único: posee información sobre su ubicación en la Tierra. No es el único tipo de dato que tiene particularidades, por ejemplo las **series de tiempo** tienen información sobre un específico período de tiempo donde se registró la información. Esto trae importantes consideraciones al momento de realizar el análisis estadístico, lo que generó el desarrollo de toda una rama de la estadística. No obstante, los **datos espaciales** no presentan un desafío solo al momento de su análisis, sino que presentan específicidades en la forma de representar su información geográfica y realizar transformaciones en los datos.

## ¿Dónde estamos en la Tierra?

La respuesta a esta pregunta puede ser un poco más compleja de lo que uno piensa, al menos si desea realizar un análisis con esta información. La respuesta más fácil en este momento sería decir: Av. Figueroa Alcorta 7350, Ciudad de Buenos Aires, Argentina. Bien, es un primer paso. Ahora: ¿Cómo calculamos la distancia con respecto a Abbey Road 2, Londres, Inglaterra, donde se encuentra el famoso cruce peatonal de la tapa del disco de los Beatles, _Abbey Road_? Imposible saberlo solo con esa información.

Si nosotros introdujéramos esos datos en un GPS (o _Google Maps_), lo que haría es _traducir_ las direcciones que les pasamos a un sistema de grillas que divide al globo en celdas en base a líneas imaginarias en sentido paralelo a los polos (paralelos) y perpendicular a ellos (meridianos). Nuestra dirección quedaría transformada directamente en un vector con dos posiciones: **latitud** y **longitud**. Ahora "Av. Figueroa Alcorta 7350, Ciudad de Buenos Aires, Argentina" se convirtió en (-34.714656,-58.785999) y "Abbey Road 2, Londres, Inglaterra" en (51.532068, -0.177305). Las latitudes y longitudes se expresan en **grados**, así que ya podemos establecer una diferencia cuantitativa entre nuestras dos posiciones ¡Incluso podemos expresarlo en una medida de distancia como metros o kilómetros!

Para esta clase vamos a necesitar varios paquetes, así que los cargamos. Recordemos que si no están instalados hay que usar la función `install.packages()`

```{r message=FALSE , tidy=TRUE, tidy.opts=list(width.cutoff=60) }
library(tidyverse) # Paquete multiuso
library(sf) # Paquete clave para manipular datos espaciales
library(leaflet) # Uno de los paquetes para 
```

Una vez que los cargamos, vamos a crear nuestro dataframe con datos espaciales en base a las coordenadas latitud y longitud que definimos anteriormente:

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Creamos un Data Frame con los datos necesarios
datos <- data.frame(lat = c(-34.714656, 51.532068),
                    long = c(-58.785999, -0.177305),
                    ubicacion = c("UTDT", "Abbey Road"))
# Lo convertimos a un objeto sf
puntosEspaciales <- st_as_sf(datos, 
                             coords = c("long", "lat"),
                             crs = 4326)
# st_distance() nos permite encontrar la diferencia en la unidad que diga el CRS (sistema de coordenadas de referencia)
st_distance(puntosEspaciales) # En metros
st_distance(puntosEspaciales)/1000 # En kilómetros
```

Según estos cálculos, nos separan aproximadamente 11.131 kms de Abbey Road. Perfecto, pudimos definir nuestra ubicación en la tierra e incluso medir la distancia con otro punto. Hay un parámetro que todavía no introdujimos y que resulta clave cuando lidiamos con datos espaciales: **CRS**, las siglas de Coordinate Reference System.

Incluso podemos hacer nuestro primer gráfico interactivo de una manera muy rápida

```{r}
leaflet(puntosEspaciales) %>% 
  addTiles() %>% 
  addMarkers()
```

Vayamos paso por paso. En la siguiente sección veremos los distintos modelos de la tierra que usamos para poder representar estas ubicaciones espaciales.

## Coordinate Reference Systems

### Elipsoides, sistemas de coordenadas y datums

Representar una ubicación en la superficie de la tierra implica superar diversos obstáculos. Para empezar, la tierra no es una esfera: tiene una forma que suele modelarse como **geoide**, pero incluso eso es una aproximación. La tierra tiene una forma particular, con diversos accidentes geográficos que la hacen única (y difícil de manipular matemáticamente). Sin embargo, nosotros - y a fines prácticos, todas las personas que trabajan con datos georreferenciados - trabajamos en su versión como **geoide**, y es en relación a esta modelización de la tierra que se montan los CRS.

Definido el **geoide**, ese modelo de la forma de la tierra, introducimos el primer componente de los CRS: el **elipsoide**. El elipsoide es una aproximación al **geoide** con mejores propiedades matemáticas. Para definir un **elipsoide** necesitamos un par de parámetros que definen su forma. Una vez que contamos con un **elipsoide** podemos establecer un sistema de grillas tridimensional, como el de latitud y longitud, que lo segmenta según los ángulos que se forman entre la línea imaginaria paralela a los polos (paralelos) y la línea imaginaria perpendicular a los polos (meridiano) en un determinado punto, en relación al paralelo y meridiano de origen.

Pero ¿cómo relacionamos al **elipsoide** con el **geoide**? Si bien el primero es una aproximación del segundo, para establecer un CRS necesitamos saber como se relacionan entre ellos: tenemos que "fijar" el **elipsoide** al **geoide**. Esto es lo que hace el **datum**: define el origen y la orientación de los ejes de coordenadas. Piensen en el **datum** como la información necesaria para "dibujar" el **sistema de coordenadas** en el **elipsoide**

Entonces ya tenemos tres elementos que poseen los CRS:

1. Un **elipsoide** (un modelo de la tierra, en rigor de un  **geoide**)
2. Un **sistema de coordenadas**, que nos permite determinar la posición de un punto en relación a otro en base a líneas imaginarias
3. Un **datum**, que nos permite dibujar ese **sistema de coordenadas** en el **elipsoide** de tal manera que represente al ubicaciones específicas en el **geoide**

Si no quedó del todo claro no se preocupen: es un tema complejo que, en la mayoría de los casos, solo basta con saber que estos conceptos existen y qué significan. El objetivo de esta subsección es dar la definición básica de cada elemento porque probablemente se encuentren con esta información en diversos lugares, pero a fines prácticos suele utilizarse siempre el mismo elipsoide, datum y sistema de coordenadas, o variaciones que no tienen grandes efectos a los fines prácticos de nuestros trabajos. El World Geodetic System (**WGS84**) es un standard en la industria a nivel mundial, y existen algunas variaciones locales (la más famosa, el North American Datum (**NAD83**)) que no nos traerán mayores problemas al momento de las transformaciones. Piensen en el CRS como las unidades de peso o de distancia: cada observación que veamos de datos espaciales corresponde a un determinado CRS y no corresponde hacer operaciones entre dos observaciones pertenecientes a distintos CRS.

### Proyecciones

Hasta ahora hemos trabajado en la representación de la Tierra en tres dimensiones. Sin embargo, todos los mapas con los que hemos trabajado desde chicos tienen dos dimensiones ¿Cómo transformamos un objeto de tres dimensiones a uno de dos dimensiones? Debemos realizar **proyecciones** de ese objeto tridimensional que, como veremos en breve, involucra diversos **tradeoffs**[^1]. Piensen en la proyección como una tarea de traducción: algo se pierde en el proceso.

La proyección hoy en día más famosa es **MERCATOR**, la proyección que usa, entre otros servicios, **Google Maps**. Diseñada hace ya varios siglos para la navegación marítima, esta transformación es relativamente buena en lo relativo preservar formas y útil para navegar.

En lo que realmente falla este tipo de proyección es en definir el tamaño de las unidades geógraficas: los países que están cerca de los polos aparentan tener un tamaño mucho más grande del que realmente tienen, mientras que lo inverso sucede con los que están cerca de la línea del ecuador. Tal es así que existe una página web (https://thetruesize.com/) que permite experimentar de manera interactiva con los tamaños de los países en diversas partes de la proyección. En la Figura 1 muestro un ejemplo: Groenlandia, Islandia, Noruega, Suecia, Finlandia y Reino Unido combinadas ocupan aproximadamente el 50% de Brasil (Figura 1).

```{r out.width="800px",echo=FALSE, fig.cap="La proyección MERCATOR distorsiona nuestra percepción de los tamaños", fig.align="center", fig.pos="htb!"}
knitr::include_graphics(path = "data/Tutorial 3/projections.png")
```

La oferta de proyecciones es prácticamente ilimitada. El paquete `mapproj` en R nos permite transformar el mundo en base a diversas proyecciones, incluyendo algunas que preservan el tamaño de los países. La Figura 2 muestra el mundo desde otra perspectiva: los países del norte son más chicos de lo que parecen en la proyección mercator.

```{r out.width="800px",echo=FALSE, fig.cap="La proyección MOLLWEIDE mantiene la representación de los tamaños", fig.align="center", fig.pos="htb!"}
knitr::include_graphics(path = "data/Tutorial 3/mollyout.png")
```

Las proyecciones también forman parte de los CRS, que pueden o no tener una proyección. Sea como sea, lo importante de esta sección es haberlos convencido de que importa **conocer en que CRS están expresados los datos espaciales**. Las transformaciones entre CRS no hace falta conocerlas, sino que el paquete **sf** lo hará por nosotros. Insisto: lo importante es saber que los datos espaciales SIEMPRE tienen un CRS, aun si no está definido explícitamente en nuestro archivo. Volvamos al ejemplo de los inmuebles de las propiedades de la introducción de este libro para ver un qué formato de archivos tienen los datos espaciales y un ejemplo sobre transformación de CRS.

## Manos a la obra ¿Dónde construir el próximo centro de salud?

Mostremos algunas de las funciones de datos espaciales de R con un problema muy concreto. Pónganse en la piel de una funcionara pública que debe decidir en qué manzana específica de la Ciudad de Buenos Aires debe abrir un nuevo centro de salud. Existen múltiples maneras de lidiar con este problema, pero supongamos por un segundo que esta funcionaria sabe como trabajar con GIS y, específicamente, con R. Antes de escuchar las demandas de los habitantes, prefiere conocer, en base a los datos, en qué manzanas hay una mayor necesidad de construir un nuevo centro de salud. Para eso, va a utilizar distintos conjuntos de datos y herramientas de R.

El objetivo va a ser generar un Índice de Demanda de Salud (IDS) que, para cada manzana de la Ciudad de Buenos Aires, nos va a indicar qué tan necesaria es la construcción de un centro de salud. Este indicador se va a basar en distintas variables, a saber:

- La densidad poblacional en la zona
- La cantidad de hogares con Necesidades Básicas Insatisfechas (NBI)
- La atención preexistente por el sistema de salud
- La distancia con la avenida más cercana

Veamos de dónde podemos conseguir estos datos y cómo podemos leerlos en R

### Cargando los datos espaciales

Los datos espaciales, a diferencia de otros tipos de datos, tienen formatos específicos para su almacenamiento. No es el objetivo de este libro introducir a todos los formatos, que son efectivamente muchos. Vamos a leer datos de tres formatos distintos: 

- **Geojson**: Se trata de una forma de representación de datos como listas, siguiendo el formato de *json*, pero adaptado para almacenar datos espaciales. Por default, el sistema de coordenada de referencias es el EPSG número 4325, que utiliza el WGS 84, el standard más utilizado en el mundo. El hecho de que lo guarde por default con este CRS es muy importante: no hace falta información de contexto sobre en qué CRS está, porque solo puede estar en ese-

- **Shapefiles**: Los shapefiles son un formato antiguo para almacenar datos espaciales, y es propiedad de la empresa ESRI (los creadores de ArcGIS). Los shapefiles están siempre compuestos múltiples archivos, cada uno cumpliendo una función. Los datos específicos de las coordenadas se encuentran en un archivo .shp, pero la proyección se encuentra en .proj, y no siempre viene incluida en nuestros datos.

- **csv**: los famosos archivos separados por comas pueden tener información espacial dentro dellos, particularmente cunado se trata de puntos. Suelen especificarse coordenadas como columnas "X" e "Y" o "lat" y "lon"


El GCBA ofrece un dataset espacial con todas las vías de circulación de autos en la Ciudad de Buenos Aires (en este link)[https://data.buenosaires.gob.ar/dataset/calles]. Les va a dar la opción de descargar los datos en shapefiles o geojson. Descarguen el shapefile (estará comprimido en un .zip) y extraiganlo en la carpeta del proyecto.

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(sf)
calles <- read_sf("data/callejero-rar/callejero.shp")
```

Para leer a este y al resto de los archivos espaciales - con excepción de csv - podemos usar **read_sf()**. Acá estoy suponiendo que tienen a todos los archivos de este shapefile dentro de la carpeta callejero-rar/

```{r message=FALSE, warning=FALSE, eval=FALSE}
# Cargamos la librería de SF
library(sf)
calles <- read_sf("callejero-rar/callejero.shp")
```

Si lograron hacerlo, entonces deberían tener un objeto que se llama **calles**. Lo primero que tenemos que entender cuando leemos datos espaciales es el sistema de coordenadas de referencia en el que están representados. Esto podemos hacerlo con **st_crs()**

```{r}
st_crs(calles)
```

En este caso nuestro dataset de calles está expresado en el EPSG 4326 ¿Qué significa esto? El EPSG es un sistema con índices de sistema de coordenadas de referencia. Cada uno de estos números representa a cada combinación de parámetros que determinan un sistema de coordenadas de referencia. Pueden buscar estos números en https://spatialreference.org/. [Aquí pueden ver específicamente información sobre el 4326](https://spatialreference.org/ref/epsg/wgs-84/).


Ahora carguemos información sobre la población y la cantidad de hogares con necesidades básicas insatisfechas - una forma de medir la pobreza - por radio censal según el censo 2010, siendo los radios censales la mínima unidad de medida del censo. Nuevamente vamos a poder descargar estos datos desde [aquí](https://data.buenosaires.gob.ar/dataset/informacion-censal-por-radio). Esta vez descarguen la opción de **geojson** y, nuevamente, guárdenlo en la carpeta de su proyecto

```{r message=FALSE, warning=FALSE, echo=FALSE}
radiosCensales <- read_sf("data/caba_radios_censales.geojson")
```

```{r message=FALSE, warning=FALSE, eval=FALSE}
radiosCensales <- read_sf("caba_radios_censales.geojson")
```

En este caso no necesitamos saber cuál el es el sistema de coordenada de referencias... siempre será el WSG84. Veamos qué variables trae

```{r}
glimpse(radiosCensales)
```

¿Ven esa columna **geometry**? Es donde están almacenados nuestros datos espaciales, en este caso son polígonos. Es una forma muy prolija de guardar la información espacial, al mismo tiempo que el resto de las variables pueden trabajarse como si fuera un data.frame normal. Si están atentos y atentas se van a dar cuenta de que no tenemos una medida de densidad... todavía, ya lo vamos a solucionar.

Finalmente, nos falta información sobre la oferta actual de cobertura de salud, para esto vamos a usar solamente a los hospitales de la Ciudad de Buenos Aires, siendo conscientes de que existen otras formas de brindar atención de salud, como los Centros de Salud y Acción Comunitaria (CeSACs).

El dataset de hospitales se encuentra en un csv al que le hice pequeñas modificaciones y pueden descargar haciendo [click acá](https://raw.githubusercontent.com/martinmontane/AnalisisEspacialEnR/master/data/Hospitales.csv)

```{r message=FALSE, warning=FALSE, echo=FALSE}
hospitales <- read_delim("data/Hospitales.csv",delim = ";")
```

```{r message=FALSE, warning=FALSE, eval=FALSE}
hospitales <- read_delim("Hospitales.csv",delim = ";")
```

Todavía esto no es un dato espacial... al menos no para R: para eso tenemos que convertirlo en un objeto SF, que tenga a las coordenadas en la columna **geometry**. Veamos qué columnas tiene

```{r}
glimpse(hospitales)
```

Entre todas las variables, **long** y **lat** tienen las variables espaciales. Podemos convertir este data.frame en un objeto sf, es decir espacial, de la siguiente manera:

```{r}
hospitales <- st_as_sf(hospitales,coords=c("long","lat"), crs=4326)
```

Nos falta el último de nuestros datos: las manzanas. Pueden [descargar el geojson de acá](https://data.buenosaires.gob.ar/dataset/manzanas)

```{r message=FALSE, warning=FALSE, echo=FALSE}
manzanas <- read_sf("data/manzanas.geojson")
```

```{r message=FALSE, warning=FALSE, eval=FALSE}
manzanas <- read_sf("manzanas.geojson")
```

Ahora sí ya podemos aplicar las distintas herramientas de análisis espacial para elegir las mejores ubicaciones para el nuevo centro de salud.

### Detectando la cobertura actual.

Una de las variables que dijimos que ibamos a tomar para detectar un espacio para un nuevo centro de salud iba a ser que la zona no estuviera atendida por un hospital. Existen distintas maneras de cubrir esta atención existente, pero vamos a usar una herramienta en particular: **st_buffer()**. Lo que hace esta función de sf es generar un polígono a una distancia fija desde cualquiera de los puntos de nuestros objetos espaciales. En el caso de puntos - como nuestros hospitales - esto significa un círculo del radio que nosotros querramos.

Una recomendación cuando trabajemos con funciones que requieren medir distancias y relaciones entre distintos objetos espaciales es que trabajemos con CRS que se encuentren proyectados en dos dimensiones, en lugar del ESPG 4326 que se encuentra no proyectado. Usemos la proyección oficial del GCBA, cuya definición podemos encontrar acá https://spatialreference.org/ref/sr-org/8333/.

Vamos a usar la función **st_transform()**, que es la que nos permite transformar entre sistemas de coordenadas de referncia. No vamos a usar los códigos de EPSG, sino la representación proj4string, que la pueden ver en el link que puse anteriormente.

```{r}
hospitales <- st_transform(hospitales, crs="+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs")
```

Si usan **st_crs(hospitales)** van a ver que es distinto ¿Ven la parte de +units=m? Bueno, eso implica que ahora todo lo que hagamos en términos espaciales será tomado en metros, por lo cual podemos encontrar círculos a una distancia fija, por ejemplo, 1000 metros, de cada hospital

```{r}
coberturaHospitales <- st_buffer(hospitales,dist = 1000)
```

Como alguno de los circulos se van a solapar, mejor juntemos todo en un gran polígono que diga que en esa zona hay cobertura, con herramientas de tidyverse

```{r}
coberturaHospitales <- coberturaHospitales %>% summarise(cobertura=TRUE) 
```

Dejemos por este dataset cargado en nuestra sesión de R por un rato, ya vamos a volver a él

### Distancia contra la avenida más cercana

Definitivamente es importante tener una buena conexión con la red de transporte para poder mover mejor a los eventuales pacientes. Teniendo nuestras calles, podemos seleccionar solo las avenidas y despues usar **st_distance()** para estimar la distancia entre cada una de nuestras manzanas y las avenidas.

```{r}
avenidas <- calles %>% filter(tipo_c == "AVENIDA")
```

Antes de usar las avenidas, lo que vamos a hacer es quedarnos con los centroides de las manzanas. Los centroides son el punto central dentro de un polígono. Hacemos esto especialmente por las manzanas que son muy grandes, para tener un "promedio" en lugar de uno solo de sus puntos externos. Es muy fácil tener los centroides, utlizando st_centroid(). Antes de hacerlo, vamos a convertir a manzanas al crs de hospitales, es decir el de la ciudad de buenos aires:

```{r}
manzanas <- st_transform(manzanas,crs = st_crs(hospitales))
```

Ahora vamosa extraer los centroides de cada una de las manzanas y hacemos un gráfico para verlo en detalle:

```{r message=FALSE, warning=FALSE}
manzanasCentroides <- st_centroid(manzanas)
ggplot() +
  geom_sf(data = manzanas) +
  geom_sf(data = manzanasCentroides,color="red", size= 0.001)
```

Si no se distinguen los puntos en este gráfico, hagánlo en su computadoras y van a poder cambiar el zoom para ver cómo cada punto está en el centro de cada manzana. Ahora midamos la distancia entre cada uno de estos puntos y todas las avenidas, pero tienen que estar en el mismo Sistema de Coordenadas de Referencia:

```{r}
avenidas <- st_transform(avenidas,crs = st_crs(hospitales))
```

Ahora sí, solo hay que esperar un poco, puede tardar unos minutos:
```{r}
distanciaAvenidas <- st_distance(manzanasCentroides,avenidas)
```

Lo que acaba de hacer es computar la distancia, en metros, entre cada uno de los centroides y las avenidas, con dim podemos ver que efectivamente esto es lo que sucedió

```{r}
# 12.520 filas (centroides de manzanas) y 6,758 columnas (tramos de avenidas)
dim(distanciaAvenidas)
```

Sin embargo, nuestro objetivo es tener la distancia mínima por lo cual podemos hacer uso de **apply**. Es una función muy poderosa que recorre nuestra matriz en filas o culmnas haciendo algo que queremos. Por ejemplo, podemos recorrer todas las filas y tomar el valor mínimo, con lo cual nos quedaríamos con el valor mínimo para cada uno de los centroides de manzanas.


```{r}
# 1 significa filas, 2 columnas. functon(x) min(x) significa que para cada fila devuelva el valor mínimo
avenidaMasCercana <- apply(distanciaAvenidas,1,function(x) min(x))
# Rendondeamos
avenidaMasCercana <- round(avenidaMasCercana,0)
```

Agregamos esta distancia a cada una de las manzanas

```{r}
manzanas <- manzanas %>% mutate(distanciaAvenida=avenidaMasCercana)
```
 
### Incorporando el resto de las variables

Ahora ya podemos agregar a los centroides, y luego a las manzanas, la información que nos falta para la construcción del índice. Para empezar, podemos hacer lo más fácil, que es marcar si el centroide está dentro de los radios de cobertura que dijimos anteriormente. Para esto, usamos **st_intersects()**, que nos dice en cuáles de los casos en los cuales el 

```{r}
manzanasCentroides <- st_join(manzanasCentroides,coberturaHospitales)
```

Igual que el **left_join()**, cuando no encuentra ningun matcheo entre las unidades devuelve **NA**, por lo que todos los valores que están en cobertura como NA en rigor no están cubiertos por la oferta de hospitales actual. Dejamos eso en claro:

```{r}
manzanasCentroides <- manzanasCentroides %>% mutate(cobertura=ifelse(is.na(cobertura),FALSE,cobertura))
```

Ahora tenemos que hacer algo muy similar con los datos del censo 2010, que lo tenemos en el objeto **radiosCensales**.

```{r}
radiosCensales <- radiosCensales %>% mutate(densidadPob=POBLACION/AREA_KM2)
radiosCensales <- radiosCensales %>% st_transform(radiosCensales,crs=st_crs(hospitales))
manzanasCentroides <- st_join(manzanasCentroides,radiosCensales)
```

Nos queda agregar toda esta información a los polígonos de las manzanas y ya podemos ponernos a construir, finalmente,nuestro índice

```{r}
# Elegimos solo las variables que queremos unir, antes lo convertimos en data frame para poder perder la columna geometry
manzanasCentroides <- manzanasCentroides %>% 
                      as.data.frame() %>%
                      select(SM,densidadPob,HOGARES_NBI,cobertura)
manzanas <-  left_join(manzanas,manzanasCentroides,by="SM")
```

### Creando nuestro índice de demanda de salud


Nuestro índice de demanda de salud va a estar compuesto por un promedio ponderado de variables que construimos, pero anteriormente deberíamos normalizarlas. Lo que vamos a hacer es ponerle un valor del 1 al 5 si está entre el 0%-20%, 20%-40%, 40%-60%, 60%-80% u 80%-100% de esa variable. Esto lo podemos hacer fácilmente con la función **quantile()**, que calcula justamente estos quiebres, veamos:

```{r}
# Queremos que nos muestre en que porcentaje de estos está cada observación...
quiebres <- c(0,0.2,0.4,0.6,0.8,1)

manzanas <- manzanas %>%
            mutate(cat_densidad=cut(densidadPob,breaks = quantile(densidadPob,quiebres,na.rm = TRUE ),include.lowest = TRUE),
                   cat_NBI=cut(HOGARES_NBI,breaks = quantile(HOGARES_NBI,quiebres,na.rm = TRUE ),include.lowest = TRUE),
                   cat_distanciaAv=cut(-distanciaAvenida,breaks = quantile(-distanciaAvenida,quiebres,na.rm = TRUE ),include.lowest = TRUE))
```

Vamos parte por parte. La función **cut()** corta a una variable númerica por los cortes que nosotros le indiquemos en el parámetro *breaks*. Por otro lado, la función **quantile()** nos devuelve los valores de una variable numérica que alcanza a un determinado porcentaje cuando se ordena de mayor a menor (es decir, calcula el percentil). Como buscamos los puntos que acumulan 0%, 20%, 40%, 60%, 80% y 100%, por los valores que le ponemos al vector quiebres, entonces nos va a devolver esos valores. **na.rm=TRUE** dentro de quantile() es para que ignore los casos donde hay NA en la variable, mientras que **include.lowest = TRUE** es para que cut tome al primer valor, 0, y no lo excluya de la nueva variable categórica.

Además, ponemos **-distanciaAvenida** porque es un truco para que nos quede como valor más alto (5) cuando la distancia es menor a la avenida y más bajo (1) cuando la distancia es mayor a la avenida.

Si todavía no quedó del todo claro, un gráfico habla más que mil palabras

```{r}
ggplot() +
  geom_sf(data=manzanas %>% filter(!is.na(cat_NBI)) ,aes(fill=cat_NBI), color=NA) +
  scale_fill_viridis_d() +
  theme_minimal() +
  coord_sf(datum=NA)
```

Bien, ahora una última vuelta de tuerca: podemos convertir estas categorías a números con la función **as.numeric()**, y estarán ordenados de 1, menor valor, a 5, mayor valor ¡Exactamente lo que queríamos! Esto es por cómo funcionan los **factores** en R

```{r}
# En el caso de cobertura convertira 0 cuando era FALSE y 1 cuando era TRUE
manzanas <- manzanas %>%
            mutate(cat_densidad=as.numeric(cat_densidad),
                   cat_NBI=as.numeric(cat_NBI),
                   cat_distanciaAv=as.numeric(cat_distanciaAv),
                   cat_cobertura=as.numeric(!cobertura))
```

Ya podemos crear nuestro índice con las ponderaciones que queramos. Usemos un 10% para la densidad, un 30% para los NBI, un 10% para la distancia con la avenida más cercana y un 50% sobre si en el lugar falta o no cobertura.

```{r}
manzanas <- manzanas %>%
            mutate(IDS=cat_densidad*0.1+cat_NBI*0.3+cat_distanciaAv*0.1+cat_cobertura*0.5,
                   IDS=ifelse(IDS>quantile(IDS,probs = 0.9,na.rm = TRUE),TRUE,FALSE))
```

Hagamos nuestro gráfico:

```{r}
# Podemos agrupar a los radioscensales por barrio para que nos queden los polígonos de los barrios.
# Tambien es posible bajarlos directamente desde la página del GCBA
barrios <- radiosCensales %>% group_by(BARRIO) %>% summarise(n())
ggplot() +
  geom_sf(data=barrios) +
  geom_sf(data=manzanas %>% filter(!is.na(IDS)) ,aes(fill=IDS), color=NA) +
  scale_fill_manual(values = c(NA,"#f03b20")) +
  guides(fill=FALSE) +
  theme_minimal() +
  coord_sf(datum=NA) +
  labs(title="Índice de Demanda de Salud",
       subtitle="Ciudad de Buenos Aires")
```


## Ejercicio

La Ciudad de Buenos Aires cuenta con otras alternativas a los hospitales, por ejemplo los CESAC. Descargá los datos de los CESAC y agregalos al gráfico donde se encuentran las áreas más acuciantes del IDS ¿Coinciden las zonas? ¿No? ¿Dónde construirían el próximo CESAC, ahora que saben dónde están emplazados los actuales? 


<!--chapter:end:DatosEspaciales.Rmd-->

# Geocoding: de la representación humana al sistema de coordenadas

En capítulos anteriores ya descubrimos la particularidad de la representación de los datos espaciales: los ubicamos en base a un modelo de la tierra en lo qu se conoce como Coordinate Reference System (CRS). Esta forma de representación es muy distinta a la que tenemos en nuestras cabezas cuando nos dicen que tenemos que ir a cursar a Figueroa Alcorta 7350. En diversas circunstancias vamos a necesitar convertir esta direcciones a puntos espaciales para poder agregar información relevante para nuestros análisis y encontrar nuevos patrones en nuestros datos.
Veamos cómo podemos hacerlo usando servicios del Estado de Argentina y también de Google.

## ¿Qué es la geocodificación o geocoding?

Geocoding no es otra cosa que la transformación de una ubicación en el formato que manejamos a diario hacia una coordenada en un sistema de coordenadas de referencia (CRS, en inglés). Esta simple operación es sumamente útil para muchos de nuestros objetivos. Imaginemos que queremos tener alguna medida de la cobertura de atención para la salud en la Ciudad de Buenos Aires. Tenemos la dirección de distintos hospitales y centros de salud, pero no conocemos cómo se distribuyen en el espacio. Carguemos primero estos datos

```{r eval=TRUE, message=FALSE}
library(tidyverse)
salud <- read_csv("data/HospitalesYCentrosSalud.csv")
```

Y veamos un poco las variables que tenemos

```{r}
glimpse(salud)
```

Geocodificemos con la ayuda del paquete **wrapar**

```{r message=FALSE,results=FALSE}
# Si no lo tenés insalado
# require(devtools)
# install_github("martinmontane/wrapar")
library(wrapar)
# Agregamos una variable de ID y una columna que indique la provincia
salud <- salud %>%
         mutate(id=row_number(),
                provincia="Ciudad de Buenos Aires")
saludGeoreferenciado <- geocodeDirecciones(datos = salud,
                                           col_id = "id",
                                           col_direccion = "Dirección",
                                           col_provincia = "provincia")
# Seleccionamos las que tuvimos algún match
saludGeoreferenciado <- saludGeoreferenciado %>% 
                        filter(nMatchAPI %in% 1)
# Le agregamos información que estaba en el anterior data.frame
saludGeoreferenciado <- left_join(saludGeoreferenciado,
                                  salud,
                                  by=c("id"))
library(sf)
# Convertimos a objeto sf
saludSf <- st_as_sf(saludGeoreferenciado,
                    coords=c("ubicacion.lon","ubicacion.lat"),
                    crs=4326)

```

Con la ayuda de leaflet hagamos un simple mapa interactivo

```{r}
library(leaflet)
leaflet(saludSf) %>% 
  addTiles() %>%
  addMarkers(label = ~ Establecimiento,
             popup = ~ Tipo)
```

Nada mal, no? No se preocupen si no entienden lo que hicimos, la única idea de esta introducción es mostrarle lo que van a ser capaces de hacer, nada más ni nada menos. Vamos a ir explicando cómo funciona todo esto.

## API: Interfaz de programación de aplicaciones

Sin saberlo, en el ejemplo anterior usamos la [API de georreferenciación de Argentina](https://datosgobar.github.io/georef-ar-api/) a través de la función **geocodeDirecciones()** del paquete wrapar ¿Qué es una API? Una API es un conjunto de reglas preestablecidas que nos permiten comunicarnos con servicios que están escritos en diferente lenguaje y con un conjunto de procedimientos específicos. Imaginenlo como que nuestro código de R es español, y la API de geolocalización del Gobierno está en francés, la API podría ser un idioma intermedio, como el inglés, para comunicarnos. Si quieren aprender como comunicarse sin hacerlo por intermedio de **wrapar** [pueden aprenderlo leyendo los documentos del desarrollo del gobierno]. La idea de wrapar es no tener que aprender otro idioma y hacer todo desde R.

Las APIs exceden a este desarrollo particular del gobierno y casi cualquier servicio de cualquier empresa tiene una API para que distintos usuarios puedan hacer consultas sin tener que conocer específicamente cómo es que el servicio trabaja por detrás, es muy útil y eficiente. En este capítulo vamos a usar dos APIs: la del Gobierno, que ya fue presentada, y la de Google. Veamos las funcionalidades y ventajas y desventajas que cada una tiene.

## Servicio de Normalización de Datos Geográficos de Argentina

Podemos comunicarnos con el servicio de normalización de datos geográficos de Argentina mediante la función **geocodeDirecciones()** del paquete **wrapar**. La función necesita que le pasemos algunos parámetros para poder hacer bien su trabajo.

- **datos**: En este parámtro simplemente hay que poner el data.frame que tiene la información que querés georeferencair
- **col_id**: es el nombre de la columna que tiene los códigos identificadores únicos de cada uno de los puntos. La función lo hace obligatorio porque va a ser la columna que después va a ser útil para incorporar el resultado de la geocodificación.
- **col_direccion**: es el nombre de la columna donde se encuentra la dirección, sin incluid información sobre la localidad, provincia, pais, etc
- **col_provincia**: es el nombre de la columna que tiene el nombre de la provincia

Con estos cuatro parámetros, cómo hicimos anteriormente, la función pasa las dirección a la API y la API nos devuelve un data.frame con las siguientes variables:

- **id**: la columna que identifica a cada uno de los puntos
- **nMatchAPI**: nos dice cuántos resultados encontró la API para esa dirección (puede ser más de uno)
- **codigoAPI**: Un código que dice "Exito" cuando se pudo comunicar con la API o "Error" cuando hubo algún problema en la comunicación
- **calle.nombre, departamento.nombre, localidad_censal.nombre, y nomenclatura**: variables donde tenemos más información sobre la dirección que encontró la API
- **ubicacion.lat y ubicacion.lon** las coordenadas de latitud y longitud en el sistema de coordenads EPSG 4326

Ahora que ya sabemos cómo funciona, relean el código que estaba en la parte de arriba ¿Cuántos puntos pudo geolocalizar? 97. En principio, no sabemos si están bien o mal (la API puede haber devuelto simplemente un punto que no correspondía), pero podemos estar seguros que para 18 de nuestros 115 casos no encontró ninguna respuesta. Esto suele pasar cuando la API no es capaz de mapear la dirección que le pasamos con otra que si reconozca. Usando otras herramientas vistas anteriormente, hagamos un mapa de distancia mínima entre cada una de las manzanas de la Ciudad de Buenos Aires y los puntos de los establecimientos de salud.

Lo primero que tenemos que hacer para medir las distancias es tener proyectados a los CRS para que la distancia midan metros y no diferencias entre coordenadas latitud y longitud.

```{r message=FALSE}
manzanas <- read_sf("http://cdn.buenosaires.gob.ar/datosabiertos/datasets/manzanas/manzanas.geojson")
# El objeto manzanas está representado en el CRS que corresponde al ESPG 4326
st_crs(manzanas)
# El objeto de saludSf no tiene información sobre el CRS de las coordenadas, pero sabemos que es 4326
st_crs(saludSf) <- 4326
# Convertimos a los dos a la proyección que usa el GCBA
manzanas <- st_transform(manzanas,
                         crs="+proj=tmerc +lat_0=-34.629269 +lon_0=-58.4633 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs ")
saludSf <- st_transform(saludSf,
                         crs="+proj=tmerc +lat_0=-34.629269 +lon_0=-58.4633 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs ")
```

Fìjense que usamos un texto largo en lugar de un código para especificar el CRS de la Ciudad de Buenos Aires. Esto suele pasar cuando una proyección no se encuentra correctamente indexada en el catálogo de EPSG. Lo único que hace EPSG es ponerle un número a cada conjunto de parámetros que determinan un CRS (como los que vemos en el texto que usamos dentro de st_transform). Para buscar los CRS cuando no saben cual usar, les recomiendo https://spatialreference.org/

Ya podemos calcular las distancias.

```{r message=FALSE}
# Tomamos las distancias
distanciaManzanas <-st_distance(x = manzanas, y=saludSf)
# Nos quedamos con el valor mínimo de distancia entre cada punto y cada manzana
distanciaManzanas <- apply(distanciaManzanas,1,min)
# Agregamos estos datos al dataset de manzanas
manzanas <- manzanas %>%
            mutate(distMinima=distanciaManzanas)
```

Quizás la parte más rara o con la que están menos familiares es la que dice **apply(distanciaManzanas,1,min)**. La función **apply()** sirve para iterar, es decir, para realizar el mismo procedimiento para todas las filas o columnas de una matriz. Cuando ponemos 1 luego de pasar a la matriz, va a hacer todo fila por fila, si dice 2 lo hará columna por columna. *distanciaManzanas* tenía la distancia en metros de cada uno de los centroides de las manzanas contra cada uno de los puntos, por lo cual lo que hace ahí es ir fila por fila y tomar el valor mìnimo (fijense que le pedimos que aplique la función min en el ùltimo argumento). Con esto, ya podemos hacer nuestro gráfico de la accesibilidad de las manzanas a establecimientos de salud:

```{r}
manzanas <- manzanas %>%
            mutate(distCat=cut(distMinima,
                               breaks = c(0,400,600,900,max(manzanas$distMinima)),
                               labels = c("Hasta 400 metros", "Entre 400 y 600 metros", "Entre 600 y 900 metros","Más de 900 metros"),
                               include.lowest = TRUE))
ggplot(manzanas) +
  geom_sf(aes(fill=distCat),color=NA) +
  scale_fill_viridis_d(direction = -1,name="Distancia") +
  theme_minimal() +
  coord_sf(datum = NA)
```

Se observa una importante parte de la Ciudad de Buenos Aires cubierta por al menos un hospital/centro de salud a menos de 400m, con algunas manchas azules en algunas zonas, que puede o no ser explicada por los 18 puntos que nos faltaron georeferenciar. Repliquemos lo mismo, pero ahora usando el servicio de google.

## Google geocode API

Google ofrece una amplia gama de servicios para lo que ellos llaman “desarrolladors”, que excede largamente el uso de la API de Google Maps. Sin embargo, la forma para acceder a todos estos servicios tiene como origen una misma cuenta. Desde esta cuenta se pueden ir agregando diversos modulos con funcionalidades distintas. Veamos como registrar la cuenta y habilitar los servicios que necesitamos.

Vayan a https://cloud.google.com/ e ingresen con una cuenta de Google (no es necesario que usen una cuenta personal, pueden crear una nueva en caso de ser necesario). Una vez que esten logueados, van a ver un botón en la esquina derecha que dicen “Consola”. Ingresen ahí y desde el menú de navegación en la parte izquierda de la pantalla vayan a APIs y servicios. Ahí van a ver que les va a pedir que creen un proyecto: para usar una API es necesario asociar a un “proyecto” en la plataforma de Google, al que le pueden poner el nombre
que ustedes quieran.

Lo que nosotros necesitamos ahora es activar las APIs que queremos usar. Google ofrece una multitud de APIs, por lo que más simple es buscarlas en la barra que nos ofrece. Vamos a activar dos APIs: Geocoding API y Distance Matrix API. Primero habilitemos Geocoding API y veamos qué pasa. Si todo funcionó bien, deberían estar en la página de su proyecto nuevo, pero ahora en el submenú de API deberían ver que dentro de API habilitadas tienen una que dice “Geocoding API” ¡Muy bien! Abajo de ese título van a ver que les recomienda otras API en API adicionales. Elijan Distance Matrix API. Habilítenla y listo, ya tenemos las dos APIs con las que vamos a trabajar en el curso.

Ahora solo nos queda crear una especie de “contraseña” con la que vamos a vincular nuestra cuenta de Google desde R. Esto es necesario porque las API de Google son servicios pagos. Dan USD 300 de crédito inicial, pero luego hay que ingresar una tarjeta de crédito para continuar usando el servicio, aunque dan USD 200 de crédito todos los meses, el equivalente aproximadamente a 40.000 búsquedas de geocoding. Esta “contraseña” se llama credencial o "key". Pueden activarla desde el menú de la izquierda en “API y servicios”
y luego “credenciales”. Allí pueden crear una credencial, que es básicamente una clave de muchos caracteres que será necesaria para lo que sigue.

**Importante: Para que efectivamente funcione lo que sigue en esta clase tienen que asociar una tarjeta de crédito a la cuenta de Google. Si bien “regala”" USD 300 en el primer uso, y USD 200 todos los meses, hay que tener cuidado una vez que la tarjeta ha sido ingresada. Si quieren probar el uso de esta herramienta pongan quotas desde su cuenta de Google.**

Hechas las presentaciones formales, veamos cómo podemos hacer lo mismo con la funcion **geocodeGoogle()**. Esta función requiere parámetros similares a la de **geocodeDirecciones()**, veamos:

- **datos**: En este parámtro simplemente hay que poner el data.frame que tiene la información que querés georeferencair
- **col_id**: es el nombre de la columna que tiene los códigos identificadores únicos de cada uno de los puntos. La función lo hace obligatorio porque va a ser la columna que después va a ser útil para incorporar el resultado de la geocodificación.
- **cols_query**: En esta columna hay que pasar un vector character con las variables que queremos que se incluyan en la consulta
- **col_key**: nombre de la columna que tiene los datos de la "key" o "credencial" de google
- **col_region**: Este parámetro es opcional y se puede delimitar la búsqueda de google a un área. Por default, region no tiene ningún valor por lo cual busca en todo el mundo.

Veamosla en función. **Este código, así como está, no debería funcionar porque la key es incorrecta**. Deben colocar una propia para que funcione

```{r message=FALSE,error=TRUE}
# Mismas operaciones que anteriormente, pero esta vez agregamos una columna con la key y de region AR
salud <- read_csv("data/HospitalesYCentrosSalud.csv")
salud <- salud %>%
         mutate(id=row_number(),
                provincia="CABA",
                key="aknfadgnadoigdagoida",
                region="AR")
```

Ahora ya podemos hacer el geocoding con google !

```{r eval=FALSE}
saludGoogle <- geocodeGoogle(datos = salud,
                             col_id = "id",
                             cols_query =c("Dirección","provincia","Barrio"),
                             col_key = "key",
                             col_region = "region")
```

```{r eval=TRUE,echo=FALSE}
load("geocodingSaludExample.RData")
```

Un mapa rápido en leaflet para ver qué encontró:

```{r}
saludGoogle <- st_as_sf(x = saludGoogle,
                        coords=c("results.geometry.location.lng","results.geometry.location.lat"),
                        crs=4326)
leaflet(saludGoogle) %>% 
  addTiles() %>%
  addMarkers()
```

Parece bastante bien, veamos cuáles son las varaibles que nos devolvió

```{r}
glimpse(saludGoogle)
```

Wow, muchas variables. La primera columna tiene, adentro, un data.frame con información que está en el resto de las columnas, así que vamos a ignorarla. Luego tenemos la **formatted_address**, que nos permite ver cuál es la calle que efectivamente nos devolvió Google. Luego tenemos cuatro coordenadas bounds, que sirven solo cuando nos devolvió un polígono en lugar de un punto (no le vamos a prestar atención, vamos a trabajar con los puntos que haya devuelto). Luego en location.lat y location.lng tenemos específicamente la latitud y longitud de nuestra dirección. Existen otras columnas, pero con estas y **id** y **partial_match** ya podemos seguir.

Si están atentos/as a lo que estamos haciendo, van a ver que saludGoogle devolvió 118 resultados cuando mandamos 115 ! Qué fue lo que pasó? Siempre que pasa esto es porque google no encontró un resultado "perfecto" para nuestra consulta y nos devolvió más de uno. Cómo nos damos cuenta? De distintas maneras, pero **partial_match**, **status** y mirar a los duplicados nos va a servir mucho. Para empezar, hagamos esto último: veamos cuáles son los que están duplicados

```{r, eval=FALSE}
# Buscamos los duplicados
idsDuplicados <- saludGoogle %>% 
                 group_by(id) %>%
                 summarise(conteo=n()) %>%
                 filter(conteo>1) %>%
                 pull(id)
# Los vemos con View()
View(saludGoogle[saludGoogle$id %in% idsDuplicados,])
```

En "results.types" van a ver que los ids 78 y 79 son rutas en lugar de puntos, por lo cual no nos sirve: no pudo encontrar un punto que se parezca a la dirección que le pasamos. Vamos a tener que filtrar esos casos. Finalmente, el id 62 nos devuelve dos casos para la misma dirección, uno pareciera ser el hospital y otro la policía (en la misma dirección): eso es algo que no nos interesa. Entonces lo que vamos a hacer es sacar a 78 y 79 y quedarnos solo con uno de los dos del 62. Lo hacemos así:

```{r, eval=FALSE}
# Filtramos los ids y cualquier caso duplicado en id
saludGoogle <- saludGoogle %>%
               filter(!id %in% c(78,79)) %>%
               filter(!duplicated(id))

```

La función duplicated nos elimina a todas las filas que aparecen por segunda vez en el data.frame con el mismo id. En este caso, elimina la segunda vez que aparece el número 62, por lo que funcionó bien. Ahora tenemos 113 puntos georeferenciados, solo perdimos el 78 y el 79. Hagamos el mismo gráfico que hicimos anteriormente:

```{r}

# El objeto de saludGoogle no tiene información sobre el CRS de las coordenadas, pero sabemos que es 4326
st_crs(saludGoogle) <- 4326
# Convertimos a los dos a la proyección que usa el GCBA
manzanas <- st_transform(manzanas,
                         crs="+proj=tmerc +lat_0=-34.629269 +lon_0=-58.4633 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs ")
saludGoogle <- st_transform(saludGoogle,
                         crs="+proj=tmerc +lat_0=-34.629269 +lon_0=-58.4633 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs ")
# Tomamos las distancias
distanciaManzanas <- st_distance(x = manzanas, y=saludGoogle)
# Nos quedamos con el valor mínimo de distancia entre cada punto y cada manzana
distanciaManzanas <- apply(distanciaManzanas,1,min)
# Agregamos estos datos al dataset de manzanas
manzanas <- manzanas %>%
            mutate(distMinima=distanciaManzanas)

manzanas <- manzanas %>%
            mutate(distCat=cut(distMinima,
                               breaks = c(0,400,600,900,max(manzanas$distMinima)),
                               labels = c("Hasta 400 metros", "Entre 400 y 600 metros", "Entre 600 y 900 metros","Más de 900 metros"),
                               include.lowest = TRUE))
ggplot(manzanas) +
  geom_sf(aes(fill=distCat),color=NA) +
  scale_fill_viridis_d(direction = -1,name="Distancia") +
  theme_minimal() +
  coord_sf(datum = NA)
```

Bastante parecida a la geolocalización de la API del gobierno, a simple vista, pero con una mayor cobertura. Veamos, ahora, en cuanto difieren los cálculos de uno y otro dataset

## Diferencias entre las dos APIs

Podemos ver las diferencias de muchas maneras, pero poniendo los puntos con un color de cada uno no sería lo mejor, ya que no podemos ver entre cual par de puntos fue la diferencia. Hagamos algo más simple: la distancia entre cada uno de los puntos.

```{r}
distancias <- st_distance(x = saludGoogle[saludGoogle$id %in% saludSf$id,],
            saludSf[saludSf$id %in% saludGoogle$id,],
            by_element = TRUE)
distancias <- data.frame(distancia=as.numeric(distancias))
ggplot(distancias) +
  geom_histogram(aes(x=distancia)) +
  theme_minimal() +
  labs(x="Discrepancia (metros)",y="Cantidad de casos")
```

La diferencia es baja en casi todos los casos, menos uno que está en aproximadamente 900 metros. Si aceptamos una tolerancia de 200 metros de diferencia, la coincidencia es casi total, nada mal. Google tiene una ventaja, igual: devolvió todos menos dos puntos.

## Una tercera alternativa: geocode_OSM()

En general, la tarea de geocodificación requiere cierta prueba y error. No todos los servicios de geocodificación saben interpretar de la misma manera a las direcciones, y muchas veces las asignan a distintas coordenadas (este problema puede ser especialmente importante al trabajar con Google Maps, que siempre intenta devolver algún resultado). **Open Street Map (OSM)** nos brinda otra alternativa para poder hacer geocoding, y esta vez de manera gratuita y sin restricciones geográficas (es decir, funciona para distintos países).

Para usar este servicio, deben tener instalado **tmaptools** y su uso es realmente simple. Solo debemos generar una variable que tenga la dirección que vamos a consultar y luego usar **geocode_OSM()**

```{r message=FALSE, warning=FALSE}
library(tmaptools)
salud <- salud %>%
         mutate(consultaOSM=paste(Dirección,", Ciudad de Buenos Aires, Argentina",sep=""))
saludOSM <- geocode_OSM(salud %>% pull(consultaOSM),projection = 4326,return.first.only = TRUE,as.sf = TRUE)
```

A diferencia de lo que hicimos con **wrapar**, en este caso ya nos devolvió un objeto sf, podemos usar leaflet para hacer un control de que, al menos, nuestros puntos están en la Ciudad de Buenos Aires

```{r}
leaflet(saludOSM) %>% 
  addTiles() %>%
  addMarkers()
```

Puede fallar... algunos están afuera de la Ciudad de Buenos Aires ¿Tienen idea de como detectarlos ? Les doy una pista: pueden bajarse un mapa de argentina por provincia (por ejemplo de acá)[https://www.indec.gob.ar/indec/web/Institucional-Indec-Codgeo] y hacer un **st_join()** y luego filtrar solo a los que están en CABA.

## Ejercicios

Busquen un conjunto de direcciones que les interese y calculen las distancias entre ellas usando la aplicación de geolocalización de Argentina y la función de distancia del paquete sf.



<!--chapter:end:Geocoding.Rmd-->

# Tiempos de viaje y análisis de accesibilidad

Nuestros datos espaciales pueden ser enriquecidos de distintas maneras, no solo a través de la relación con otras entidades espaciales, tal como se mostró en el ejercicio de ubicación óptima para uno nuevo centro de salud anteriormente. Es posible agregar información sobre tiempos de desplazamiento en distintos tipos de desplazamiento o aproximar un polígono que, para un determinado punto, muestra cuáles son los lugares a los que se pueden acceder en u determinado tiempo. Yendo un paso más atrás, incluso podemos lograr encontrar la coordenadas a partir del texto de una dirección. Vamos a aplicar todos estos conceptos y verlos en acción en un tema  muy interesante: el acceso a los espacios verdes en la Ciudad de Buenos Aires.

## La distancia espacial y la distancia de viaje

Las personas que vivimos en ciudades entendemos a la perfección que la distancia entre punto A y B puede medirse de distintas maneras. A veces, caminar 1000 metros suele tardar mucho menos que hacerlo en auto, y a veces los tiempos de transporte público pueden variar mucho según la proximidad con distintos medios de transporte disponibles.

En distintas circunstancias puede ser muy importante entender cuáles son las condiciones de acceso de cada una de las zonas de una ciudad para distintas razones: trabajo, recreación, salud, entre otras variables. En esta clase vamos a medir el acceso a la los espacios verdes de la Ciudad de Buenos Aires, pero tan solo por una cuestión de acceso a datos. Si tuvieramos, por ejemplo, información sobre la ubicación de los puntos donde las empresas están ubicadas, podríamos estimar la accesibilidad al mercado de trabajo de cada uno de los lugares. Sea como fuere, basta de preámbulos y veamos cómo podemos procesar los datos espaciales para tener una medida espacial del acceso a espacios verdes en la Ciudad de Buenos Aires.

Para esto vamos a hacer lo siguiente: 

- Medir la cobertura de los espacios verdes de la CABA, midiendo desde qué lugares se puede llegar a 15 minutos caminando
- Cruzar estos datos con las manzanas y establecer que aquellas manzanas que no se encuentran en este espacio de cobertura están "mal atendidas"


## Los paquetes que vamos a utilizar

Los capítulos de este libro en general no suelen introducir los paquetes que tienen las herramientas que vamos a utilizar antes de que sean necesarias para resolver un problema en particular. Sin embargo, para este capítulo hacemos una pequeña excepción, ya que vamos a cargar un conjunto de paquetes en los que vamos a tener que hacer zoom tanto en lo que ofrecen como en qué pasos adicionales tenemos que hacer para utilizarlas. Vamos paso por paso:

- **sf**: Este paquete ya lo conocemos, es el que nos permite trabajar con datos espaciales en R.
- **tidyverse**: Colección de paquetes que nos permite cumplir muchas de las tareas necesarias en un proyecto de ciencia de datos. En esta clase vamos a investigar una función del paquete **purrr**, parte de tidyverse, y que nos permite
- **leaflet**: Herramienta muy poderosa para generar mapas interactivos. En este capítulo lo vamos a utilizar para que resolver uno de los potenciales problemas que puede tener R para actuar como GIS: la falta de modificaciones o exploraciones "manuales" de los datos espaciales
- **hereR**: Si bien existen distintas alternativas para medir los tiempos de desplazamiento, en este caso vamos a usar el servicio de HERE maps. La API de here puede usarse utilizando el lenguaje de R gracias a las personas que desarrollaron **hereR**.

### Cómo usar los servicios de HERE

HERE Maps es una empresa que nos brinda distintas herramientas de geolocalización, medidas de tiempo de viaje entre distintos puntos. Piensen en ella como una caja de herramientas donde podemos elegir entre ellas para poder resolver problemas particulares. A diferencia de otras alternativas, como Google Maps, HERE nos permite realizar hasta 250.000 consultas gratuitas por mes sin tener que poner nuestra tarjeta de crédito como garantía. Solía ser de esta manera con Google Maps en el pasado, pero cambiaron las condiciones de un tiempo para acá, por lo cual hay que buscar alternativas y HERE nos puede ser útil.

Para usar HERE Maps en R, además de instalar el paquete **hereR**, debemos tener una *key*, que no es otra cosa que una contraseña única que nos pide HERE Maps para poder vincular el uso que le damos a la cuenta desde R con sus registros internos. Para esto, primero hay que crear una cuenta en https://developer.here.com/. Una vez que hayan creado la cuenta - gratuita - tienen que ir a los "projects" que tiene y hacer click en **create API key** donde dice REST. Una vez que la creen, van a ver que les aparece una tabla con "API KEY" y un botón que dice "COPY". Hagan click ahí y peguénlo por algún notepad o similar, ya van a ver cómo vamos a poder utilizarlo.


## Los datos: repositorio de datos del GCBA

Nuestra materia prima para la introducción a estas herramientas estatales será el dataset espacial de espacios verdes de la Ciudad de Buenos Aires y la información sobre el trazado urbano de la Ciudad, también provisto por el GCBA. Estos datos pueden reemplazarse por otros datasets en caso que quieran replicar este análisis para otras ciudades. En particular para Argentina, es posible usar los radios censales, la mínima unidad espacial para un censo en Argentina, para reemplazar las manzanas. Por otro lado, los datasets de espacios verdes puede ser un poco más difíciles de encontrar, pero siempre existen esfuerzos para construir estos datasets que pueden encontrar haciendo búsquedas por Google

Los datos de las manzanas se pueden descargar desde https://data.buenosaires.gob.ar/dataset/manzanas, mientras que los de espacios verdes desde https://data.buenosaires.gob.ar/dataset/espacios-verdes. En mi caso, yo descargué los geojson y los guardé en una carpeta que se llama **data**, pero pueden hacer lo que ustedes crean necesario!


```{r, message=FALSE, warning=FALSE,results='hide'}
library(tidyverse)
library(sf)
library(hereR)
library(leaflet)
manzanas <- st_read("data/manzanas.geojson")
espaciosVerdes <- st_read("data/espaciosVerdes.geojson")
```

Veamos rápidamente qué es lo que cargamos con la ayuda de leaflet:

```{r}
leaflet(espaciosVerdes) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons()
```

Parece que tenemos un conjunto de espacios urbanos de la ciudad de buenos aires, que es exactamente lo que queríamos. Igualmente vamos a hacer algunos cambios a este dataset, ya que vamos a quedarnos con espacios que sean lo suficientemente grande, de una manera muy arbitraria, como para poder ser aprovechados recreativamente: vamos a imponer que el espacio verde tenga al menos una superficie de 10.000 metros cuadrados, es decir una manzana.

## Transformando nuestros datos

Ahora que tenemos ya cargados nuestros datos espaciales, podemos transformarlos como para poder medir la cobertura de la oferta de espacio verde en la CABA. Para empezar, vamos a restringir espacios verdes a aquellos espacios verdes que tengan más de 1km2 de superficie total. Esto lo hacemos porque queremos medir de alguna manera el "uso" que le pueden dar a esos espacios, no solo si existe o no un lugar con zonas verdes. Es un criterio discutible, pero acá viene la mejor parte: cuando terminen este capítulo van a poder ir cambiando estos criterios. 

¿Cómo podemos saber el tamaño de los polígonos de los espacios verdes? Esto es posible hacer siempre que tengamos nuestros datos espaciales cargados como un objeto sf. Para esto, primero vamos a proyectar en 2 dimensiones a los datos que cargamos, usando la proyección adaptada a la Ciudad de Buenos Aires:

```{r}
espaciosVerdes <- st_transform(espaciosVerdes,
                               crs = "+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs")
```

Una vez que hicimos esto, entonces ya estamos en condiciones de poder conocer el área de cada uno de los polígonos. La función que usamos es **st_area()**, pero debemos usar un punto, que hace referencia a "todo el dataset" dentro de tidyverse. Es decir, cuando usemos mutate, por ejemplo, el "." hará referencia a todas las filas, es decir, a todos los polígonos de nuestro dataset. La función st_area() nos devuelve un tipo particular de datos: **units**, es decir que son datos con unidad (en este caso, metros cuadrados). Como esto ya lo sabemos, lo convertimos en un vector clásico con **as.numeric()** como para poder luego usar **filter()** y quedarnos solos con los que queríamos

```{r}
espaciosVerdes <- espaciosVerdes %>% 
                  mutate(area=st_area(.)) %>% 
                  mutate(area=as.numeric(area)) %>%
                  filter(area>10000) 

```

Probemos nuevamente con qué logramos quedaros:

```{r}
leaflet(espaciosVerdes %>% st_transform(4326)) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons()
```

Muy bien, ahora pensemos en un problema un poco más específico. Para medir la distancia entre dos puntos necesitamos... ¡puntos! Y nuestros dos datasets, manzanas y espacios verdes, son polígonos, tenemos que resolver esto de alguna manera. Podríamos tomar centroides de ambos casos pero tendríamos un problema muy importante: cuando las figuras son largas y/o grandes, el centroide no puede ser un buen indicador de punto de partida o llegada.

La solución propuesta es solo medir el tiempo de viaje caminando desde los espacios verdes, pero desde un conjunto de puntos **al azar** que estén dentro de los polígonos, como para poder hacer más representativo el hecho de que al espacio verde se puede acceder desde distintos puntos. Si esto no queda del todo claro, vamos con un ejemplo bien sencillo. Enfoquémonos un segundo en el Parque Chacabuco. Para eso vamos a filtrarlo y convertirlo a ESPG 4326, que es lo que necesita leaflet para graficar

```{r}
parqueChacabuco <- espaciosVerdes %>% 
                   filter(nombre == "Parque Chacabuco") %>% 
                   st_transform(4326)
leaflet(parqueChacabuco) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons()
```

El parque es grande, si tomáramos una esquina, probablemente caminando 10 o 15 minutos estaríamos todavía dentro del mismo parque, y quizás arrancando desde el centro también... Retomemos este problema mientras aprendemos a pedir tiempo de viajes.

## ¿Cuánto tardo en llegar al monumental?

El equipo más grande de la Argentina tiene su estadio en Avenida Presidente Figueroa Alcorta 7597, en la Ciudad de Buenos Aires. Yo vivo por alguna zona de Palermo - vamos a mantener un poco la anonimidad - digamos Charcas 3591. Solo tengo esta información, pero me gustaría saber cuánto tiempo puedo tardar caminando, en auto o en bicicleta ¿Puedo hacerlo? Claró, puedo entrar a Google Maps y buscar las opciones. Pero acá vamos a hacerlo desde R y con el servicio de HERE Maps, lo cual después nos va a permitir llevar esto un paso más adelante y responder nuestro problema inicial.

Lo primero que tenemos que hacer es generar un data frame con la información que sí tenemos:

```{r}
# Esta key es falsa, por obvias razones. Reemplacen este valor por uno que funcione
direcciones <- data.frame(lugar=c("Casa","Monumental"),
                          direccion=c("Charcas 3591, Ciudad de Buenos Aires, Argentina","Avenida Presidente Figueroa Alcorta 7597, Ciudad de Buenos Aires, Argentina"))

```

Luego, usamos la función de **geocode()** de hereR, que solo nos pide un vector con las direcciones a geolocalizar:

```{r echo=FALSE}
load(file = 'data/ubicacionRiver.RData')
```

```{r eval=FALSE}
ubicaciones <- geocode(direcciones %>% pull(direccion))
```

Si todo salió bien, ahora deberían tener un objeto *ubicaciones* con información espacial sobre nuestros datos... veamos si están bien georreferenciados

```{r}
leaflet(ubicaciones) %>% 
  addTiles() %>% 
  addMarkers()
```

Si exploran un poco, van a ver que todo salió relativamente bien. Ahora lo que vamos a hacer es usar la función **route()**, la que se encarga de consultarle a HERE Maps cuánto tardamos entre el punto A y el B. En principio, solo necesita un origen, un destino y un modo de transporte, que en este caso va a ser "pedestrian", es decir, cuánto tardamos caminando entre los dos puntos

```{r echo = FALSE}
load("data/viajeRiver.RData")
```
```{r eval=FALSE}
viaje <- route(origin = ubicaciones[1,],
               destination = ubicaciones[2,],
               mode = "pedestrian",
               datetime = as.POSIXct(x = "17/01/2020 18:00:00",format="%d/%m/%Y %H:%M:%S"))
```
```{r}
glimpse(viaje)
```
El tiempo lo devuelve en la variable **travelTime** en segundos, por lo que dice que tardariamos 6480/60 = 108 minutos (una hora y 48 minutos) en hacer 6.389 metros (vean "distance"). Muy bien, ahora probablemente les haya aparecido un horario distinto al que aparece en la salida de este libro... y esto es porque por default hereR consulta por cuanto tardaría saliendo en el mismo instante de la consulta. No se preocupen, acá les explico como replicar lo que hice recién, y también puede serles útiles para medir otro tiempo de distancia. Solo tienen que cambiar la fecha y el horario y les va a funcionar correctamente

```{r eval=FALSE}
viaje <- route(origin = ubicaciones[1,],
               destination = ubicaciones[2,],
               mode = "pedestrian",
               datetime = as.POSIXct(x = "17/01/2020 18:00:00",format="%d/%m/%Y %H:%M:%S"))
```

Medir tiempo de viaje en auto es muy similar, solo tienen que cambiar "pedestrian" por "car", de la siguiente manera:

```{r eval=FALSE}
viaje <- route(origin = ubicaciones[1,],
               destination = ubicaciones[2,],
               mode = "car",
               datetime = as.POSIXct(x = "17/01/2020 18:00:00",format="%d/%m/%Y %H:%M:%S"))
```

Bien, ahora estamos más cerca de la herramienta que finalmente vamos a usar: las isocronas. Esto que suena horrible es simplemente **un polígono que delimita el espacio al cual se puede llegar en un tiempo fijo**. Se trata de una aproximación, ya que nunca puede saberse exactamente cuál es este polígono, pero hagamos la prueba desde mi supuesto hogar: cuál es el polígono que podemos alcanzar caminando solo 15 minutos

```{r echo=FALSE}
load("data/isolinea.RData")
```

```{r eval=FALSE}
viajeCaminando <- isoline(ubicaciones[1,],mode = "pedestrian",range = 60*15)
```

Ahora veamos qué es lo que nos devolvió:

```{r}
leaflet(viajeCaminando) %>% 
  addTiles() %>% 
  addPolygons() %>% 
  addMarkers(data=ubicaciones[1,])
```

Ese polígono nos muestra todos los lugares a los que podemos acceder en una caminata de 15 minutos desde Charcas 3591. Ahora bien, recuerden nuestro punto inicial: queremos medir la cobertura de los espacios verdes en la Ciudad de Buenos Aires. Algunos son muy grandes, entonces nos conviene conseguir esta isocrona para más de un punto dentro del mismo parque, de tal manera de poder capturar este efecto. Veamoslo, de nuevo, con el el ejemplo del Parque Chacabuco.

## Midiendo la cobertura de los parques

Si siguieron este capítulo correctamente, deberían tener un objeto **parqueChacabuco** con sus polígonos. Vamos a medir la cobertura mediante dos métodos alternativos: usando el **centroide** de los polígonos y tomando al azar 4 puntos del parque. Comparemos los resultados.

Lo primero que tenemos que hacer, es reproyectar esos polígonos para poder tomar puntos al azar y también para tomar los centroides. En general, cuando realizamos esta clase de operaciones conviene tener a los datos espaciales proyectados en dos dimensiones

```{r}
parqueChacabuco <- parqueChacabuco %>% 
                   st_transform("+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs")
```

Ahora ya podemos tomar el centroide, con la función **st_centroid**

```{r}
centroideChacabuco <- st_centroid(parqueChacabuco)
```

Y también cuatro puntos al azar, con la función **st_sample**

```{r echo = FALSE}
load("data/puntosChacabuco.RData")
```

```{r eval = FALSE}
set.seed(1)
puntosChacabuco <- st_sample(parqueChacabuco,size = 4)
```

Como siempre, podemos ver muy fácilmente que es lo que acabamos de hacer, con la ayuda de leaflet

```{r}
leaflet() %>% 
  addTiles %>% 
  addPolygons(data=parqueChacabuco %>% st_transform(4326)) %>% 
  addCircleMarkers(data=puntosChacabuco %>% st_transform(4326), color='red') %>% 
  addCircleMarkers(data=centroideChacabuco %>% st_transform(4326),color ='yellow') 
  
```

Ahora midamos la isolinea de 15 minutos para esta forma de identificar al parque. Antes de so, tenemos que hacer algo con puntosChacabuco, porque no es un objeto sf ! miren

```{r}
class(puntosChacabuco)
```

Lo que tenemos que usar es **st_as_sf()**, que es la forma de decirle a R que queremos que ese objeto sea uno sf:

```{r}
puntosChacabucoSF <- st_as_sf(puntosChacabuco)
class(puntosChacabucoSF)
```

Ahora sí, ya estamos en condiciones de hacer lo que queríamos. Para el centroide es muy simple, usamos de nuevo **isoline()** y no tenemos mayores inconvenientes

```{r echo= FALSE}
load("data/isoCronaCentroide.RData")
```

```{r eval= FALSE}
isoCronaCentroide <- isoline(centroideChacabuco,mode = "pedestrian",range = 60*15)
```

Ahora bien, lamentablemente la función **isoline()** no hace automáticamente la operación para todas las filas. Eso ciertamente nos haría el trabajo más simple, pero no vamos a impedir que eso nos deje terminar el trabajo. Lo que vamos a usar es la función **map()** del paquete **purrr**. Lo que hace es hacer una función, la que queramos, para un conjunto de objetos que le digamos. Es muy general, por lo cual veamosla en funcionamiento:

```{r}
map(c(1:5),function(x) x+5)
```

Todas las funciones de **map()** tienen las dos cosas que les dije. En primer lugar, un conjunto de elementos a los cuales queremos hacerles una función en particular. En este caso, le pasamos los números que van desde el 1 al 5. Después, creamos una función que toma el el valor x y le suma 5. **x** en ese contexto significa cada uno de los valores que decíamos antes. Lo que devuelve es una lista con todos los valores que queríamos. Si no quedó del todo claro, no importa: ya va a quedar más claro con la experiencia. Apliquemoslo a este ejemplo entonces:

```{r echo = FALSE}
load("data/isoCronaPuntos.RData")
```
```{r eval = FALSE}
isocronaPuntos <- map(1:nrow(puntosChacabucoSF),
                         function(x) { isoline(puntosChacabucoSF[x,],mode = "pedestrian",range = 60*15) })
```

Fijense que lo que nos devolvió es una lista con 4 isolineas basadas en cada uno de los puntos que tomamos aleatoriamente anteriormente. Ahora nos queda juntarlos con rbind. Podemos hacerlo uno por uno, pero también podemos apoyarnos en **do.call()** que es muy similar a **map()**. La principal diferencia en este caso es que tenemos que pasarle la función que queremos que haga entre comillas, y luego la lista sobre la que queremos que lo haga. Nosotros queremos qu use **rbind** para todos los elementos de **isocronaPuntos**.

```{r}
isocronaPuntos <- do.call("rbind",isocronaPuntos)
```

Vemos que es lo que tenemos hasta ahora:

```{r}
leaflet(isocronaPuntos) %>% 
  addTiles() %>% 
  addPolygons()
```

Podemos ver que las curvas se solapan quizás demasiado, y eso no es lo que buscamos. Entonces directamente lo que vamos a hacer es unir todo en un solo gran polígono, para hacer las cosas más fáciles. Esto se hace con **st_union()**

```{r}
isocronaPuntos <- st_union(isocronaPuntos)
```

Finalmente podemos comparar la cobertura con ambas metodologías: centroides y puntos. Veamos la diferencia:

```{r}
ggplot() +
  geom_sf(data=parqueChacabuco) +
  geom_sf(data=isoCronaCentroide, fill="red",alpha=0.1) +
  geom_sf(data=isocronaPuntos, fill="orange", alpha=0.2)
```

Otra forma de verlo es sumar el área de cobertura de ambas alternativas:

```{r}
st_area(isoCronaCentroide)
st_area(isocronaPuntos)
```

Una diferencia más que importante en cobertura ! Ahora ya estamos en condiciones de hacer el ejercicio por el que veníamos: caracterizar la cobertura de los espacios verdes en la Ciudad de Buenos Aires

## Redondeando: caracterizando la oferta de los espacios verdes en CABA

Toda la larga discusión que tiene este capítulo puede resumirse en tan pocas líneas como las que siguen:

```{r echo=FALSE}
load("data/isocronosEspaciosVerdesJuntasUnion.RData")
```

```{r eval=FALSE}
# Para cada uno de los espacios verdes agarramos 4 puntos al azar
puntosEspaciosVerdes <- map(1:nrow(espaciosVerdes),function(x){
  st_sample(espaciosVerdes[x,],size=4)
})
# Juntamos todos los puntos en un objeto
puntosEspaciosVerdes <- do.call("c",puntosEspaciosVerdes)
# Lo convertimos a un objeto SF
puntosEspaciosVerdesSF <- st_as_sf(puntosEspaciosVerdes)
# Hacemos una transformación para que esté en el WSG84, que es lo que puede procesar isoline()
puntosEspaciosVerdesSF <- st_transform(puntosEspaciosVerdesSF,crs=4326)
# Efectivamente calculamos las isocronas para cada uno de los puntos
isocronosEspaciosVerdes <- map(1:nrow(puntosEspaciosVerdesSF),function(x) {
  # Esta línea es solo para que nos vaya avisando qué está haciendo
  cat("Procesando: ",x,"\r")
  isoline(puntosEspaciosVerdesSF[x,], mode = "pedestrian",range_type = "time",range = 60*15)
})
# Los juntamos en el mismo data frame
isocronosEspaciosVerdesJuntas <- do.call(rbind,isocronosEspaciosVerdes)
# Hacemos un gran poligono
isocronosEspaciosVerdesJuntasUnion <-  st_union(isocronosEspaciosVerdesJuntas)
```

Va a tardar un poco porque son 588 puntos para los cuales tiene que encontrar las icoronas... Pero si lo dejan correr, va a terminar de procesarlo. Podemos ver la cobertura que estimamos en un mapa de leaflet

```{r}
leaflet(isocronosEspaciosVerdesJuntasUnion %>% st_transform(4326)) %>% 
  addTiles() %>% 
  addPolygons()
```

Ahora simplemente tenemos que hacer un spatial join con **st_join()**. Recuerden que lo que hace esta función es unir a dos datasets según algún criterio de unión, siendo por default si se intersectan o no. En los casos que se intersecten, entonces va a agregar la información que se encuentra en el segundo dataset al primero. En caso de que no existe unión, ese valor para ese punto/polígono en particular será **NA**. Para hacer el spatial join vamos a proyectar a los dos datasets, convertimos como objeto sf a todo el espacio que identifica la cobertura de espacios verdes, y creamos una variable que identifique eso, y generamos efectivamente el spatial join.

```{r}
# Transformamos la proyección de las manzanas
manzanas <- manzanas %>%
            st_transform("+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs")
# Transformamos la proyección de la cobertura de espacios verdes
isocronosEspaciosVerdesJuntasUnion <- isocronosEspaciosVerdesJuntasUnion %>%
  st_transform("+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs")
# Lo convertimos en un objeto sf y creamos una columna, cobertura, que tendrá valor TRUE siempre
isocronosEspaciosVerdesJuntasUnion <- st_as_sf(isocronosEspaciosVerdesJuntasUnion) %>% 
                                      mutate(cobertura=TRUE)
# Spatial join
manzanas <- st_join(manzanas,st_as_sf(isocronosEspaciosVerdesJuntasUnion))
# Completamos los datos para los casos en los cuales no hubo ningún resultadoe en el match
manzanas <- manzanas %>% mutate(cobertura=ifelse(is.na(cobertura),FALSE,TRUE))
```


Hacemos el gráfico con ggplot, listo para exportar en caso que sea necesario con **ggsave()**

```{r}
ggplot(manzanas) +
  geom_sf(aes(fill=cobertura), color=NA) +
  theme_minimal() +
  coord_sf(datum=NA) +
  scale_fill_manual(values = c("#377eb8","#e41a1c"),
                    breaks = c(TRUE,FALSE),
                    labels=c("Menos de 15 minutos","Más de 15 minutos"))
```

Se puede mejorar y hacer el siguiente gráfico si usan **ggmap()** para agregar un mapa de base. Usamos **getbb** para que nos de la Bounding Box, es decir cuatro puntos, que definen todo un rectángulo dónde se puede ver la Ciudad de Buenos Aires 

```{r message=FALSE, warning=FALSE}
library(osmdata)
library(ggmap)
bbCABA <-getbb("Ciudad de Buenos Aires, Argentina")
cabaBaseMap <- get_stamenmap(bbCABA,maptype = "toner-lite",zoom=12)
ggmap(cabaBaseMap,extent = "device") +
  geom_sf(data=manzanas %>% st_transform(4326),aes(fill=cobertura), color=NA, inherit.aes=FALSE, alpha=0.7) +
    theme_minimal() +
  coord_sf(datum=NA) +
  labs(x="",y="") +
  scale_fill_manual(values = c("#377eb8","#e41a1c"),
                    breaks = c(TRUE,FALSE),
                    labels=c("Menos de 15 minutos","Más de 15 minutos"),
                    name="")+
  theme(legend.position = "bottom")
```


## Ejercicios

1. ¿Cuánto tiempo tardás en llegar desde tu casa hasta la oficina de trabajo en auto? Usá hereR en R para poder sacar esa conclusión ¿Qué camino te sugirió? Podés descubrirlo haciendo un gráfico del objeto que devuelve la función **route()**

2. Estimar la cobertura de espacios verdes en la ciudad de buenos aires, pero en lugar de usar 15 minutos como en el capítulo, usar 30 minutos ¿Qué zonas no están cubiertas con un parque a 30 minutos de caminata?

3. El GCBA ofrece información sobre la ubicación geográfica de las (comisarias en la Ciudad)[!https://data.buenosaires.gob.ar/dataset/comisarias-policia-ciudad] ¿Hay zonas de la Ciudad en las cuales no hay acceso a una comisaria en menos de 10 minutos en auto, según HERE Maps?  


